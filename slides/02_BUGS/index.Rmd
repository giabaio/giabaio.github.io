---
title: "&#8291;2. Introduction to Bayesian computation and `BUGS`"
author: Gianluca Baio
institute: "[Department of Statistical Science](https://www.ucl.ac.uk/statistics/) | University College London"
params: 
   - conference: "STAT0019 - Bayesian Methods in Health Economics"
   - location: "UCL"
date: 
output:
  xaringan::moon_reader:
    includes: 
       in_header: "../assets/latex_macros.html" 
       ## This line adds a logo based on the format selected in the file 'assets/THEME/include_logo.html'
       ## NB: the actual options (eg placement of the logo and actual logo file) can be changed there
       ## There's also a script to manipulate the colouring scheme for the UCL logo (from a basic black/white one)
       after_body: "../assets/ucl-stats/insert-logo.html"
    seal: false
    yolo: no
    lib_dir: libs
    nature:
      beforeInit: ["https://platform.twitter.com/widgets.js"]
      highlightStyle: github
      highlightLines: yes
      countIncrementalSlides: no
      ratio: '16:9'
      titleSlideClass:
      - center
      - middle
    self_contained: false 
    css:
    - "../assets/ucl-stats.css"
---

```{r echo=F,message=FALSE,warning=FALSE,comment=NA}
# Sources the R file with all the relevant setup and commands
source("../assets/setup.R")

# Stuff from 'xaringanExtra' (https://pkg.garrickadenbuie.com/xaringanExtra)
# This allows the use of panels (from 'xaringanExtra')
xaringanExtra::use_panelset()
# This allows to copy code from the slides directly
xaringanExtra::use_clipboard()
# This freezes the frame for when there's a gif included
xaringanExtra::use_freezeframe()

# Defines the path to the file with the .bib entries (in case there are references)
bibfile=ReadBib("~/Dropbox/Perso/Office/CV/mypubs.bib",check = FALSE)
```

class: title-slide

# `r rmarkdown::metadata$title`

## `r rmarkdown::metadata$author`

### `r rmarkdown::metadata$institute`    

.title-small[
`r icons::icon_style(icons::fontawesome("envelope",style = "solid"),scale=.8,fill="#00acee")`  [g.baio@ucl.ac.uk](mailto:g.baio@ucl.ac.uk)
`r icons::icon_style(icons::fontawesome("firefox"),scale=.8,fill="#EA7600")`  [https://gianluca.statistica.it/](https://gianluca.statistica.it/)
`r icons::icon_style(icons::fontawesome("firefox"),scale=.8,fill="#EA7600")`  [https://egon.stats.ucl.ac.uk/research/statistics-health-economics/](https://egon.stats.ucl.ac.uk/research/statistics-health-economics/)
`r icons::icon_style(icons::fontawesome("github"),scale=.8,fill="black")`  [https://github.com/giabaio](https://github.com/giabaio)
`r icons::icon_style(icons::fontawesome("github"),scale=.8,fill="black")`  [https://github.com/StatisticsHealthEconomics](https://github.com/StatisticsHealthEconomics)
`r icons::icon_style(icons::fontawesome("twitter"),scale=.8,fill="#00acee")`  [@gianlubaio](https://twitter.com/gianlubaio)     
]

### `r rmarkdown::metadata$params`

<!--
Adds a departmental logo on the right-bottom corner (Only with 'ucl-stats')
-->
.logo-stats[]

<!--
Can also add sticky notes:
`r postit(text=paste0('Check out our departmental podcast "Random Talks" on Soundcloud!', add_podcast()),top="75%",left="2.5%",height="6.3em",width="6.3em")`

`r postit(text=paste0("Follow our departmental social media accounts", add_twitter(url="https://twitter.com/stats_ucl",title="@stats_UCL",fill="#00acee"), add_linkedin(url="https://www.linkedin.com/in/statistical-science-ucl-906b9a201",title="LinkedIn")),top="53%",left="6.5%",height="6.3em",width="6.3em")`
-->

<!-- This adds a footer (optional and with other possibilities...) 
     For example, can use `r samptux()` to add the Samp Tux log,
     or change the bottom space to align the text, etc
.footer-left[
<span style="position: relative; bottom: 0px; color: #D5D5D5;"> &nbsp; &copy; Gianluca Baio (UCL)</span>
]
-->


---

layout: true

.my-footer[ 
.alignleft[
&nbsp; &copy; Gianluca Baio (UCL) `r add_twitter()` `r add_github()` `r add_email()` `r add_website()`
]
.aligncenter[
`r rmarkdown::metadata$title` 
]
.alignright[
`r course_site()` &nbsp; STAT0019 
]
] 

---

# Summary 

- The Bayesian view making *probability statements about parameters*
   - *Quantities* that control reality (patient's risk of disease, treatment effects $\ldots$)
   - Don't / can't know them for sure
   - Can express this *uncertainty* using tools of *probability*

- Examples of probability distributions

- Monte Carlo simulation for prediction under uncertainty

- Implementation in `BUGS`

`r vspace("4rem")`


.content-box-beamer[
### References 
`r vspace("20px")`

`r bugs_book(c(1,2,5))`

`r bmhe(c(2,4))`
]

---

# Bayesian computation

.pull-left[
In artificially simplified modelling structures, Bayesian computations are just as easy as "standard" statistical models
   
- [Thomas Bayes](https://youtu.be/-e8wOcaascM) (1763) .alignright[`r emo::ji("backhand index pointing right")`] 
   - Set up (what we now call) a Binomial model for number of "successes" out of a set number of "trials"
   - Applied to billiard balls:  
   
- Pierre-Simon Laplace (1786)
   - Analysed data on christening in Paris from 1745 to 1770 using (what we now consider) a Bayesian model
   - Concludes that he was "morally certain" that $\Pr(`r sftext("new born is boy")`\mid `r sftext("data")`) \geq 0.5$ (divine providence to account for the fact that males died at higher rates...)
]

.pull-right[
<iframe width="560" height="315" src="https://www.youtube.com/embed/-e8wOcaascM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
]

---

count: false
name: video
# Bayesian computation

.pull-left[
But they can become very complicated in realistic models

- Alan Turing (1940s): [breaking the Enigma code](https://youtu.be/oTltncUkckQ) .alignright[`r emo::ji("backhand index pointing right")`] 
   - Using "prior" information and guess a stretch of letters in an Enigma message, measure their belief in the validity of these guesses and more clues as they arrived
       
Since the 1990s, rely on computer simulations and a suite of algorithms called **Markov Chain Monte Carlo** (MCMC)

- Highly generalisable, can throw at it virtually any complexity

- Can still be computationally intensive, but variants of "vanilla" implementations can be made **very** efficient
]
.pull-right[
<iframe width="560" height="315" src="https://www.youtube.com/embed/oTltncUkckQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
]

--

`r vspace("20px")`

Need to manipulate probability distributions to express both .red[**sampling variability**] and .olive[**epistemic uncertainty**]!

- Choose most appropriate distribution for the quantity of interest


---

# Beta distribution

General distribution representing uncertainty about a *proportion* or *probability* - values between *0 and 1*

.pull-left[
```{r, echo=F,fig.width=5,fig.height=5,out.width='105%',fig.align='center',dev='tikz',message=FALSE,warning=FALSE,cache=TRUE,opts=list(title="A Beta distribution is extremely versatile to represent uncertainty on quantites defined in the range 0-1. Upon varying the parameters of the distribution, we can get very different shapes indicating different levels of knowledge or uncertainty about the underlying value of the variable modelled using the Beta distribution",width="95%",dev="tikz")}
theta=seq(0,1,.001)
a=c(.5,5,1,15,5,150)
b=c(.5,5,1,5,1,50)
d=lapply(1:length(a),function(i) dbeta(theta,a[i],b[i]))
par(mfrow=c(3,2))
for (i in 1:length(a)) {
  plot(theta,d[[i]],t="l",lwd=3,xlab="",ylab="",axes=F,main="")
  mtext(side=1,line=2.5,"$\\theta$",cex=1.15)
  mtext(side=2,line=2.5,"Density",cex=1.0)
  mtext(side=3,line=1.5,paste("$a=",a[i],",b=",b[i],"$"),cex=1.25)
  axis(1);axis(2)
}
```
]

.pull-right[.center-right[
$\theta \sim `r sftext("Beta")`(a,b)$ has density
$$ p(\theta\mid a, b) = \frac{\Gamma(a+ b)}{ \Gamma(a)\Gamma(b) } \,\,\theta^{a-1}  (1-\theta)^{b-1} $$
Mean and variance:
\begin{align} 
\mu & =\frac{a}{a+ b }  \\\\
\sigma^2 &=  \frac{a b}{ (a+ b)^2 (a+ b+1)}
\end{align}   
]
]

---

count: false
name: beta-tricks
# Beta distribution

## Expressing beliefs as a Beta distribution

1. Define mean and SD

   - $`r sftext("Beta")`(a,b)$ has mean $\displaystyle \mu = \frac{a}{a+b}$, variance $\displaystyle \sigma^2 = \frac{ab}{(a+b)^2(a+b+1)}$
   - Solving gives $a,b$ in terms of assumed mean and SD:
   $$ a = \mu\left(\frac{(1 - \mu)\mu}{\sigma^2} - 1\right)$$
   $$ b = (1 - \mu)\left(\frac{(1 - \mu)\mu}{\sigma^2} - 1\right )$$
   eg mean 0.4, sd 0.1 gives Beta(9.2, 13.8)
  
--

2. Use an *implicit* dataset
   - Imagine your beliefs about the success rate $p$ are equivalent to observing, e.g. $y_0=8$ successes out of $n_0=20$ trials
   - This gives about $`r sftext("Beta")`(y_0+1, n_0-y_0+1)$ &ndash; see `r ref_lecture("mcmc","conjugacy")` for theory!
   - Mean $(y_0+1)/(n_0+2) \approx 0.4$ and SD $\approx 0.1$ here
   
---

count: false
# Beta distribution

.pull-left[
```{r echo=F,fig.width=5,fig.height=5,out.width='100%',fig.align='center',dev="tikz",message=FALSE,warning=FALSE,cache=TRUE,opts=list(title="Again, changing the values for the parameters a and b does modify the shape of the underlying Beta distribution. If we select a=20 and b=25, this is essentially equivalent to assuming a thought experiment that represents our uncertainty before seeing any data to a trial with 25 people of whom 25 experienced the event under consideration. The shape of the distribution is similar if we consider a=200 and b=250 (same as before but 10 times bigger). This is because we're now considering a prior based on an experiment with 200 'successes' out of 250 trials. So the underlying proportion of successes is the same (20/25=200/250), but we are now more confident, because we pretend to have observed 10 times as many people. This explains why the Beta(1,1) indicates complete ignorance - because it is akin to a thought experiment with 0 successes out of 0 trials")}
theta=seq(0,1,.001)
par(mfrow=c(2,2))
y=8;n=20
main=paste0("$y_0=",y,",n_0=",n,"$")
plot(theta,dbeta(theta,y+1,n-y+1),t="l",lwd=2,xlab="$\\theta$",ylab="Density",axes=F,main=main)
axis(1);axis(2)

y=20;n=25
main=paste0("$y_0=",y,",n_0=",n,"$")
plot(theta,dbeta(theta,y+1,n-y+1),t="l",lwd=2,xlab="$\\theta$",ylab="Density",axes=F,main=main)
axis(1);axis(2)
abline(v=(y+1)/(n+2),lty=2,lwd=2,col="blue")
lines(c(qbeta(.025,y+1,n-y+1),qbeta(.975,y+1,n-y+1)),c(0,0),lwd=4,col="blue")

y=200;n=250
main=paste0("$y_0=",y,",n_0=",n,"$")
plot(theta,dbeta(theta,y+1,n-y+1),t="l",lwd=2,xlab="$\\theta$",ylab="Density",axes=F,main=main)
axis(1);axis(2)

y=0; n=0
main=paste0("$y_0=",y,",n_0=",n,"$")
plot(theta,dbeta(theta,y+1,n-y+1),t="l",lwd=2,xlab="$\\theta$",ylab="Density",axes=F,main=main)
axis(1);axis(2)
```
]

.pull-right[
```{r,comment=NA,warning=FALSE,message=FALSE}
# Sets prior "successes" and "trials" 
y=20; n=25

# Computes the mean of the distribution
mean(rbeta(1000000,y+1,n-y+1))

# Computes summary statistics 
cbind("2.5%"=qbeta(.025,y+1,n-y+1),
      "median"=qbeta(.5,y+1,n-y+1),
      "97.5%"=qbeta(.975,y+1,n-y+1))
```


**NB**: $y_0=n_0=0 \Rightarrow$ "complete ignorance" *non-informative* prior &ndash; Beta(1,1) = Uniform(0,1)
]

---

name: gamma-distribution
# Gamma distribution

*Positive*, skewed quantities (eg costs, variance parameters)

.pull-left[
```{r echo=F,fig.width=5,fig.height=5,out.width='100%',fig.align='center',dev="tikz",message=FALSE,warning=FALSE,cache=TRUE,opts=list(width="95%",title="A Gamma is a skewed, positive distribution with two parameters a and b. Like for the Beta, changing their values gives us different shapes")}
maxval=c(5,15,5,15,5,40)
y=lapply(1:length(maxval),function(i) seq(0,maxval[i],.01))
a=c(1,3,0.1,30,3,10)
b=c(1,.5,.1,5,3,.5)
d=lapply(1:length(a),function(i) dgamma(y[[i]],a[i],b[i]))
par(mfrow=c(3,2))
for (i in 1:length(a)) {
  plot(y[[i]],d[[i]],t="l",lwd=3,xlab="",ylab="",axes=F,main="")
  mtext(side=1,line=2.5,"$y$",cex=1.15)
  mtext(side=2,line=2.5,"Density",cex=1.0)
  mtext(side=3,line=1.5,paste("$a=",a[i],",b=",b[i],"$"),cex=1.25)
  axis(1);axis(2)
}
```
]

.pull-right[.center-right[
If $Y \sim `r sftext("Gamma")`(a,b)$

\begin{align}
p(y\mid a, b) & = \frac{ b^a }{\Gamma(a) } y^{a-1} \; e^{-by}   \\\\
{\rm E}(Y \mid a, b) &= \frac{a}{ b }\\\\
{\rm V}(Y \mid a, b) &= \frac{a}{ b^2 }
\end{align}


`r icon::icon_style(icon::fontawesome("info-circle"),fill="blue")` log-Normal distribution is similar (but heavier tails than Gamma &ndash; see also `r ref_lecture("ild","gamma-lognormal")`!)
]
]

---

# *Doing* Bayesian analysis

## Broadly speaking, there are two types of Bayesian analysis:

**Forward sampling**  (Monte Carlo): this lecture
- Express current knowledge as parameters with distributions     
- Simulate parameters, make predictions from models based on the parameters     
- Like a spreadsheet with randomness on cells. Familiar in health economics as "probabilistic sensitivity analysis" (**see `r ref_lecture("intro_he")`**)    
- Doesn't really need specialised software

--

`r vspace("2em")`

**Model fitting** using Bayes Theorem (Markov Chain Monte Carlo): **see `r ref_lecture("mcmc")`** 
- Combine prior knowledge with *learning from data*    
- Searches for the unknown *posterior* distribution based on prior and data    
- Needs to use/programme specific software (eg `BUGS`)

---

count: false
# *Doing* Bayesian analysis

## The `BUGS` language

### `B`**ayesian analysis** `U`**sing** `G`**ibbs** `S`**ampling**

- Language for specifying Bayesian models as a *network* of *known and unknown* quantities


.pull-left[
eg linear regression
\begin{align}
Y_i & \sim `r sftext("Normal")`(\mu_i, \tau)\\
\mu_i & = \alpha + \beta x_i\\
\tau & = 1 / \sigma^2
\end{align}

```{r comment=NA,prompt=FALSE,eval=FALSE}
for (i in 1:N) {
  Y[i] ~ dnorm(mu[i], tau)
  mu[i] <- alpha + beta*x[i]
}
tau <- 1/(sigma*sigma)

# Prior knowledge 
alpha ...
beta ...
sigma ...
)
```
]

.pull-right[
Works by internally constructing *directed acyclic graph* - model code *equivalent* to a graph

<center><img src="img/line.png" width="650"></center>

Simulates distributions of unknowns conditional on prior distributions and data
]

---

count: false
# *Doing* Bayesian analysis 

### Different versions of `BUGS` 

- `WinBUGS` 1.4.3 
   - Original release 1997 &ndash; runs only on `Windows`
   - Stable but no longer developed (latest release: August 2007)
   - Freely available from <img src="img/WinBUGSlogo.jpg"; width="15"> [http://www.mrc-bsu.cam.ac.uk/bugs](http://www.mrc-bsu.cam.ac.uk/bugs)

- `OpenBUGS` <img src="img/openbugs.jpg"; width="15"> [http://www.openbugs.net](http://www.openbugs.net) 
   - Open-source offshoot, also runs on `Linux` and `MacOS`
   - Works just as well, stable

--

### "Rivals"/alternatives 

- `JAGS` [http://mcmc-jags.sourceforge.net](http://mcmc-jags.sourceforge.net) 
   - Language essentially identical, Work just as well, stable 
   - Runs natively on `Mac`/`Unix`/`Windows`

- `Stan` <img src="img/stan_logo.png"; width="15"> [http://mc-stan.org/](http://mc-stan.org/)
   - *Probabilistic* language &ndash; slightly different than `BUGS`/`JAGS`
   - Based on different algorithm &ndash; can be more efficient in some cases


Interfaces exist to run these from other software, eg `r icon::fontawesome("r-project")`  ([`R2OpenBUGS`](https://cran.r-project.org/web/packages/R2OpenBUGS/R2OpenBUGS.pdf), [`R2jags`](https://cran.r-project.org/web/packages/R2jags/R2jags.pdf), [`rstan`](https://cran.r-project.org/web/packages/rstan/rstan.pdf)) `Excel`, `S-Plus`, `SAS`, `Matlab`, `Stata`, ...


---

count: false
# *Doing* Bayesian analysis 

## A Bayesian workflow...

1. Pre-process data

   - Import dataset from spreadsheet
   - Create new variables
   - Subset data
   - ...

--

2. Sets up model
   - Code up distributional assumptions
   - Define initial values (**more on this in `r ref_lecture("mcmc")`!**)

--

3.  "Run" the model and obtain simulations from the target (posterior) distributions
   - Typically done using "Gibbs Sampling" (eg `BUGS`)
   - But other engines/MCMC algorithms/approximation methods available

--

4. Use simulations to summarise results
   - Make histograms of target (posterior) distributions
   - Compute means, sd, medians, quantiles of target (posterior) distributions
   - Derive distributions for functions of original parameters $g(\theta)$ &ndash; **more on this in `r ref_lecture("ald")`!** 
   
---

# Some aspects of the `BUGS` language

Not a conventional programming language 

- `BUGS` is for *describing a model*
   - ***not*** for performing a sequence of tasks in order

- Every line defines a model quantity in relation to others  

> ### Random (stochastic) dependence
>
> e.g.:  <tt style = "font-family:inconsolata; color: #143455;">r ~ dunif(a,b)</tt> 

- Simulate data $r$ from model, or fit model to observed data $r$

> ### Fixed (logical) dependence
>
> e.g.: <tt style = "font-family:inconsolata; color: #143455;">m <- a + b*x</tt>

- Define any quantity as deterministic function of another (as in a spreadsheet)

---

# Functions in the `BUGS` language

Use in definitions of logical quantities, eg mathematical functions

- .red[`tau <- 1 / pow(s,2)`] sets $\tau = 1/s^2$
- .red[`s   <- 1 / sqrt(tau)`] sets $s = 1/\sqrt{\tau}$

Useful data processing tricks, eg
- .red[`p <- step(x - 0.7)`]  = 1 if `x` $\ge$ 0.7, 0 otherwise. Hence monitoring `p` and recording its mean will give the
probability that `x` $\ge$ 0.7
- .red[`p <- equals(x, 0.7)`]  = 1 if `x` $=$ 0.7, 0 otherwise


See "Model Specification/Logical nodes" in manual for full syntax

---

# Some common distributions

```{r echo=F,message=F,error=F}
library(knitr)
library(kableExtra)
tab=tibble(
  expression=c("dbin","dnorm","dpois","dunif","dgamma"), 
  distribution=c("Binomial","Normal","Poisson","Uniform","Gamma"), 
  usage=c("r ~ dbin(p, n)","x ~ dnorm(mu, tau)","r ~ dpois(lambda)","x ~ dunif(a, b)","x ~ dgamma(a, b)")
)
tab %>% kable(col.names=c("Expression","Distribution","Usage")) %>% 
  kable_classic() %>% 
  kable_styling(full_width = F) %>% 
  column_spec(c(1,3),extra_css="font-family: 'Inconsolata';") %>% 
  row_spec(0,bold=TRUE,extra_css="border-top: 2px solid;") %>% 
  row_spec(5,extra_css="border-bottom: 2px solid;")
```

`r vspace("1em")`

- **NB**: The normal is parameterised in terms of its mean and ***precision*** = 1/ variance = $1/\sd^2$ 

- Functions cannot be used as arguments in distributions (you need to create new nodes)

See "Model Specification/The `BUGS` language: stochastic nodes/Distributions" in manual for full syntax

---

# Arrays and loops in `BUGS`

Use arrays and loops for sets of related quantities

```{r comment=NA,eval=FALSE,prompt=FALSE}
for (i in 1:n) {
  r[i] ~ dbin(p[i],n[i])
  p[i] ~ dunif(0,1)
}
```

Array functions: eg `mean(p[])` to take mean of whole array, `mean(p[m:n])` to take mean of elements `m` to `n`.  Similarly, `sum(p[])`


See "Hints on using `OpenBUGS`" handouts, or the `OpenBUGS` manual for full information or `BUGS` syntax

---

# Forward sampling

- Consider a drug to be given for relief of chronic pain
- Experience with similar compounds has suggested that annual response rates between 0.2 and 0.6 could be feasible
- Interpret this as a distribution with mean = 0.4 and standard deviation = 0.1

```{r echo=FALSE,comment=NA,warning=FALSE,error=FALSE,message=FALSE,fig.width=7,fig.height=5,out.width="50%",fig.align='center',dev="tikz",opts=list(width="48%",title="A Beta with parameters a=9.2 and b=13.8 encodes the assumption that the mean is about 0.4 and that most of the distribution is included in the interval between 0.2 and 0.6")}
plot(seq(0,1,.001),dbeta(seq(0,1,.001),9.2,13.8),t="l",axes=F,xlab="Probability of success",ylab="")
axis(1)

# a=9.2
# b=13.8
# ggplot(data.frame(x = seq(0,1,.001)), aes(x = x)) + 
#   stat_function(fun = dbeta, args = list(shape1 = a, shape2 = b), size=2) + 
#   stat_function(fun = dbeta, args = list(shape1 = a, shape2 = b), 
#                 xlim = c(0,qbeta(.025,a,b)), 
#                 geom = "area", fill = "steelblue", alpha = .2) +
#   stat_function(fun = dbeta, args = list(shape1 = a, shape2 = b), 
#                 xlim = c(qbeta(.975,a,b),1), 
#                 geom = "area", fill = "steelblue", alpha = .2) +
#   scale_x_continuous(name = "$\\theta$", breaks = seq(0,1,.2)) +
#   scale_y_continuous(name = "") + 
#   theme_classic() +
#   theme(axis.ticks.y=element_blank(),
#         axis.text.y=element_blank(),
#         axis.line.y = element_line(size = 0.5, linetype = "solid", colour = "white")
#   )
```

A **.red[Beta(9.2,13.8)]** distribution has these properties 

---

count: false
# Forward sampling 
`r vspace("-20px")`
### Prior *knowledge* vs prior *distribution*

- Consider a drug to be given for relief of chronic pain
- Experience with similar compounds has suggested that annual response rates between 0.2 and 0.6 could be feasible
- Interpret this as a distribution with mean = 0.4 and standard deviation = 0.1

```{r echo=FALSE,comment=NA,warning=FALSE,error=FALSE,message=FALSE,fig.width=7,fig.height=5,out.width="50%",fig.align='center',dev="tikz",opts=list(width="48%",title="But a Normal distribution with parameters mean=-0.40 and sd=0.41, when rescaled on the logit scale, encodes pretty much the exact same information!")}
ln=rnorm(10000,logit(.4),0.413)
hist(ilogit(ln),main="",xlim=c(0,1),xlab="Probability of success",ylab="",freq=F,axes=F,col="dark grey",border="light grey")
axis(1)
points(seq(0,1,.001),dbeta(seq(0,1,.001),9.2,13.8),t="l",lwd=3,col="blue")
legend("topright",c("$\\mbox{log}\\left(\\frac{\\theta}{1-\\theta}\\right) \\sim \\mbox{Normal}(-0.405,0.413)$"," ","$\\theta \\sim \\mbox{Beta}(9.2,13.8)$"),
       bty="n",col=c("dark grey","white","blue"),lty=1,lwd=7,cex=1.28)
```

**But**: there are many possible ways to encode this prior information. For example, one could use a **.red[Normal distribution on the logit scale]**!

---

# Forward sampling

## Making predictions

1. Model sampling variability of $y\mid\theta$ and uncertainty on $\theta$
2. **Propagate** uncertainty in the success rate $\theta$
3. Compute the *.red[predictive]* distribution of $y$: averaged over uncertainty about $\theta$

.myblue[$$p(y) = \int p(y\mid\theta) p (\theta) d\theta$$]

--

For instance, we can model 
- $y\mid \theta\sim `r sftext("Binomial")`(\theta,n)$: "natural" model for sampling variability
- $\theta \sim `r sftext("Beta")`(a,b)$: convenient model for epistemic uncertainty

In `BUGS`:
```{r comment=NA,prompt=FALSE,eval=FALSE}
model {
  theta ~ dbeta(a, b)
  y ~ dbin(theta, n)
}
```

---

count: false 

# Forward sampling .subtitle[Making predictions]

.panelset[
.panel[
.panel-name[`R` Code]
```{r echo=TRUE}
# Sets up the parameters for the Beta prior
a=9.2
b=13.8

# Then simulates 1000 random samples from the prior
theta=rbeta(n=10000,a,b)

# Now "mixes" **uncertainty** in the value of the parameter 
# with **variability** in the data sampling distribution
y=rbinom(n=10000,size=20,prob=theta)
```
]
.panel[
.panel-name[Output]
.pull-left[
```{r echo=FALSE,dev="tikz",fig.width=6,fig.height=5,opts=list(width="90%",title="The prior distribution is defined on the scale of the parameter, so in this case the range 0-1 (because the parameter is a probability of success)")}
plot(seq(0,1,.01),dbeta(seq(0,1,.01),a,b),t="l",axes=F,xlab="$\\theta$",ylab="$p(\\theta)$",main="(a)")
axis(1); axis(2)
```
]
.pull-right[
```{r echo=FALSE,dev="tikz",fig.width=6,fig.height=5,opts=list(width="90%",title="The predictive distribution is defined on the scale of the actual data (in this case the positive integers up to 20)")}
barplot(prop.table(table(y)),xlab="$y$",ylab="$p(y)$",main="(b)")
```
]

(a) is the Beta (prior) distribution 

(b) is the predictive **.olive[Beta-Binomial]** distribution of the number of successes in the next 20 trials
]
]

---

exclude: true

# Forward sampling .subtitle[Making predictions]

.pull-left[
```{r echo=FALSE,comment=NA,warning=FALSE,error=FALSE,message=FALSE,fig.width=6,fig.height=5,dev="tikz",opts=list(width="85%",title="INSERT TEXT HERE")}
plot(seq(0,1,.001),dbeta(seq(0,1,.001),9.2,13.8),t="l",axes=F,xlab="Probability of success",ylab="")
axis(1)
```
]
.pull-right[
```{r echo=FALSE,comment=NA,warning=FALSE,error=FALSE,message=FALSE,fig.width=6,fig.height=5,dev="tikz",opts=list(width="85%",title="INSERT TEXT HERE")}
theta=rbeta(10000,9.2,13.8)
ypred=rbinom(10000,20,theta)
tab=table(ypred)
h=hist(ypred,plot=FALSE)
barplot(h$density,names.arg=h$mids-.5,xlab="Number of successes")
```
]


(a) is the Beta (prior) distribution for $\theta$

(b) is the predictive **.olive[Beta-Binomial]** distribution of the number of successes $y$ in the next 20 trials

---

# Forward sampling 

## Using MC to estimate tail area probabilities

- What is the chance of getting 15 or more responders?
   - $\theta \sim `r sftext("Beta")`(9.2, 13.8)$: prior distribution
   
   - $y \sim `r sftext("Binomial")`(\theta, 20)$: sampling distribution
   
   - $P_{\rm crit} = \Pr(y \ge 15)$: probability of exceeding critical threshold


--

- In `BUGS` can code by translating the equations
```{r comment=NA,eval=FALSE,prompt=FALSE}
# In BUGS syntax
 model {
   theta ~ dbeta(9.2, 13.8)   # prior distribution
   y ~ dbin(theta, 20)        # sampling distribution
   P.crit <- step(y - 14.5)   # = 1 if y >=15, 0 otherwise
 }
```

- **NB**: in `BUGS`, statements can be given in any order!

---

# Forward sampling 

## `OpenBUGS` output and exact answer

```{r comment=NA,eval=FALSE,prompt=FALSE}
 node     mean    sd     MC error    2.5%    median  97.5% start sample
 theta   0.4008  0.09999 9.415E-4    0.2174  0.3981  0.6044  1   10000
 y       8.058   2.917   0.03035     3.0     8.0     14.0    1   10000
 P.crit  0.0151  0.122   0.001275    0.0     0.0     0.0     1   10000
```

**NB**: Mean of the 0-1 indicator `P.crit`: estimated tail-area probability

Exact answers from closed-form algebra:
- $\theta$:  mean 0.4 and standard deviation  0.1
- $y$:       mean 8 and standard deviation  2.92
-  Probability of at least 15:  0.015

MC error $\approx$ $`r sftext("sd")`/\sqrt{`r sftext("No. iterations")`}$ = std. error for *estimate of* mean

Can achieve arbitrary accuracy by running the simulation for longer

---

# Forward sampling 

## `OpenBUGS` output

`r include_fig("drug-MC.png",width="650",title="The BUGS output can show different metrics to assess the results, including traceplots (more on this in the next lecture) and histograms or densities for the relevant quantities")`

Independent samples, so no concern with convergence (More on this later...)

---

# `OpenBUGS` script

## Drug: Monte Carlo example

Run from `Model/Script` menu
.small[
```{r comment=NA,eval=FALSE,prompt=FALSE}
modelDisplay('log')                      # set up log file
modelCheck('c:/bugscourse/drug-MC')      # check syntax of model
#  modelData('c:/bugscourse/drug-data')  # load data file if there is one
modelCompile(1)                          # generate code for 1 simulations
# modelInits('c:/bugscourse/drug-in1',1) # load initial values if necessary
modelGenInits()                          # generate initial values for all unknown 
                                         # quantities not given initial values
samplesSet(theta)                        # monitor the true response rate
samplesSet(y)                            # monitor predicted number of successes
samplesSet(P.crit)                       # monitor whether a critical number of successes
samplesTrace("*")                        # watch some simulated values (NB: slows simulation!)
modelUpdate(10000)                       # perform 10000 simulations
samplesHistory(theta)                    # Trace plot of samples for theta
samplesStats("*")                        # Summary statistics for all monitored quantities
samplesDensity(theta)                    # Plot distribution of theta
samplesDensity(y)                        # Plot distribution of y
```
]

Automates mouse clicks: important analyses should be *.red[repeatable]* (**But** effectively superseded by more effective tools, eg `R2OpenBUGS`!)


.small[(warning: script commands different between `OpenBUGS` and `WinBUGS`)]

.small[.alignright[`r icon::fontawesome("arrow-circle-right")` [Next lecture](../03_MCMC/index.html)]]
